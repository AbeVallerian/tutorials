{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bahasa Indonesia POS Tagger using BLSTM-CNN-CRF\n",
    "I construct a network based on [this paper](https://arxiv.org/abs/1603.01354) for Bahasa Indonesia part-of-speech (POS) tagger. The idea is to use word embedding and also character embedding as features to be inputted to LSTM. Before LSTM, character vectors are going to Convolution and Max Pooling layers. Then, CRF layer is used to predict the sequence.\n",
    "\n",
    "The workflow is as follows:\n",
    "1. [Preprocessing Data](#Preprocessing-Data): I do data preprocessing mostly on numpy, instead of doing it on tensorflow. Besides word and character vectors, I also use word shape as feature.\n",
    "2. [Building the Network](#Building-the-Network): On top of my network, I also use Dropout.\n",
    "3. [Training the Network](#Training-the-Network): I use mini-batch Adam optimizer with learning rate decay and gradient clipping.\n",
    "4. [Evaluating the Network](#Evaluating the Network)\n",
    "\n",
    "I add the summary for Tensorboard as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library\n",
    "Note that [Sastrawi](https://github.com/har07/PySastrawi) is the stemmer library for Bahasa Indonesia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import time\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "The tagged Bahasa Indonesia is obtained from [here](https://github.com/famrashel/idn-tagged-corpus). It contains around 10k labeled sentences. Also, the list of tagset with the description can be found [here](http://bahasa.cs.ui.ac.id/postag/downloads/Tagset.pdf).\n",
    "\n",
    "First, I extract the token and label of each sentence. Then, I use the stemmer for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize stemmer\n",
    "stemmer = StemmerFactory().create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences: 10030\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = open('INPUT-THE-FILE-LOCATION-HERE', 'rb') \\\n",
    "                        .read().split('\\n\\n')\n",
    "print '#sentences:', len(raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1723f61d8b1d40a58cb1a9a404ddd164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = list() #list of raw tokenized sentences \n",
    "tags = list() #list of labels\n",
    "stem_sentences = list() #list of stemmed tokenized sentences\n",
    "for sentence_ in tqdm(raw_sentences):\n",
    "    sentences_ = list()\n",
    "    tags_ = list()\n",
    "    stem_sentences_ = list()\n",
    "    for words_tags_ in sentence_.split('\\n'):\n",
    "        _ = words_tags_.split('\\t')\n",
    "        sentences_.append(_[0])\n",
    "        tags_.append(_[1].upper())\n",
    "        stem_sentences_.append(stemmer.stem(_[0]))\n",
    "    sentences.append(sentences_)\n",
    "    tags.append(tags_)\n",
    "    stem_sentences.append(stem_sentences_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use **gensim's Word2Vec** to create initial word embedding for each token. Although the word embedding is trained during the network training, Word2Vec provides better starting point than random initialization. I use *min_count=2* to provide representation for OOV token, since token with only 1 occurence will be removed. Note that I use label 0 for pad token and 1 for unknown token.\n",
    "\n",
    "Then, I preprocess each sentence with post padding and post truncation so each sentence has equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vocab: 6737\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec(stem_sentences, sg=1, size=128, min_count=2, seed=100)\n",
    "print '#vocab:', len(w2v.wv.vocab)\n",
    "word_vec_embed = np.vstack((np.zeros(128), np.zeros(128), w2v.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {token_: idx_+2 for idx_, token_ in \\\n",
    "                              enumerate(w2v.wv.index2word)}\n",
    "word2index['<PAD>'] = 0\n",
    "word2index['<UNK>'] = 1\n",
    "index2word = {idx_: token_ for token_, idx_ in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 30\n",
    "index_sentences = list()\n",
    "for sentence_ in stem_sentences:\n",
    "    index_sentences.append([word2index[token]\n",
    "                            if token in word2index else 1\n",
    "                            for token in sentence_])\n",
    "index_sentences = tf.contrib.keras.preprocessing.sequence \\\n",
    "                    .pad_sequences(index_sentences, SEQ_LENGTH,\n",
    "                                   padding='post', truncating='post')\n",
    "#sentence length will be used in calculating loss and prediction\n",
    "seq_length = np.array([min(SEQ_LENGTH, len(sentence_))\n",
    "                       for sentence_ in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using character embedding, character feature from OOV token can still be extracted. \n",
    "\n",
    "Actually I can use gensim to provide the initial character embedding (just like I do with word), but I prefer not to since I don't think it will be significant. Hence, I use random initialization for character embedding. Note that, just like word embedding, I use label 0 for pad character and 1 for character token.\n",
    "\n",
    "Also, I preprocess each word with post padding and post truncation so each word has equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2index = {char_: idx_+2 for idx_, char_ in\n",
    "              enumerate(list(string.lowercase))}\n",
    "for i in range(10):\n",
    "    char2index[str(i)] = i+28\n",
    "char2index['<PAD>'] = 0\n",
    "char2index['<UNK>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_LENGTH = 15\n",
    "def convert_token(token):\n",
    "    if token == '<PAD>': #all zero sequence\n",
    "        return tf.contrib.keras.preprocessing.sequence \\\n",
    "                    .pad_sequences([[0]], TOKEN_LENGTH,\n",
    "                                   padding='post', truncating='post')\n",
    "    else:\n",
    "        chars = [char2index[char] if char in char2index else 1\n",
    "                 for char in token]\n",
    "        return tf.contrib.keras.preprocessing.sequence \\\n",
    "                    .pad_sequences([chars], TOKEN_LENGTH,\n",
    "                                   padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sentences = list()\n",
    "for sentence_ in stem_sentences:\n",
    "    char_sentence = list()\n",
    "    count = 0\n",
    "    for token_ in sentence_:\n",
    "        char_sentence.append(convert_token(token_))\n",
    "        count += 1\n",
    "        if count == SEQ_LENGTH-1:\n",
    "            break\n",
    "    for i in range(SEQ_LENGTH-count):\n",
    "        char_sentence.append(convert_token('<PAD>'))\n",
    "    char_sentences.append(char_sentence)\n",
    "#the output is reshaped to fit input of convolution layer\n",
    "char_sentences = np.array(char_sentences).reshape(-1, SEQ_LENGTH,\n",
    "                                                  TOKEN_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I build label encoder for POS tag and the label sequence has to be preprocessed with padding and truncation as well. Note that 0 is the label for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tag: 24\n"
     ]
    }
   ],
   "source": [
    "tag_list = ['<PAD>']\n",
    "for tag_ in tags:\n",
    "    tag_list += tag_\n",
    "tag_list = set(tag_list)\n",
    "print '#tag:', len(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_encoder = LabelEncoder().fit(list(tag_list))\n",
    "index_tags = list()\n",
    "for tag_ in tags:\n",
    "    index_tags.append(list(tag_encoder.transform(tag_)))\n",
    "index_tags = tf.contrib.keras.preprocessing.sequence \\\n",
    "                    .pad_sequences(index_tags, SEQ_LENGTH,\n",
    "                                   padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I include word shape as the additional feature and preprocess the sequence of word shape with padding and truncation also. Note that 0 is the label for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_shape(token):\n",
    "    if re.sub('[^A-Za-z\\s]+', '', token) != token:\n",
    "        return 1 #other\n",
    "    elif token.lower().title() == token:\n",
    "        return 2 #upperInitial\n",
    "    elif token.lower() == token:\n",
    "        return 3 #lowercase\n",
    "    elif token.upper() == token:\n",
    "        return 4 #uppercase\n",
    "    else:\n",
    "        return 5 #mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_word_shapes = list()\n",
    "for sentence_ in sentences:\n",
    "    _ = [word_shape(token) for token in sentence_]\n",
    "    sentence_word_shapes.append(_)\n",
    "sentence_word_shapes = tf.contrib.keras.preprocessing.sequence \\\n",
    "                        .pad_sequences(sentence_word_shapes,\n",
    "                        SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I split train (80%) and test (20%) set. Actually, I have to prepare dev set as well, but I don't that it's necessary for my case. So basically, my test set is dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, X_ws_train, X_ws_test, \\\n",
    "    seq_length_train, seq_length_test, X_char_train, X_char_test = \\\n",
    "        train_test_split(index_sentences, index_tags,\n",
    "                         sentence_word_shapes, seq_length,\n",
    "                         char_sentences, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network\n",
    "Now, I start building the network. Since this is a quite complex network, there are so many hyperparameters. I also write the hyperparameter values to tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting the graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 100\n",
    "TAG_SIZE = len(tag_list) #number of POS tag\n",
    "SHAPE_SIZE = np.max(sentence_word_shapes) + 1 #number of wordshape type\n",
    "DATA_SIZE = X_train.shape[0] #size of train data\n",
    "CHAR_SIZE = len(char2index) #number of character type\n",
    "CHAR_DIM = 32 #dimension of character embedding\n",
    "COMB_CHAR = 5 #convolution filter size\n",
    "OUT_CHAR = 8 #output dimension from convolution layer\n",
    "LSTM_DIM = 128 #output dimension of LSTM layer\n",
    "KEEP_PROB = 0.5 #on dropout\n",
    "LEARNING_RATE = 1e-2\n",
    "DECAY_RATE = 0.95 #on learning rate\n",
    "DECAY_STEP = 50 #on learning rate\n",
    "GRAD_MAX = 5.0 #for gradient clipping\n",
    "EPOCHS = 16 #number of iterations; can be manually interrupted\n",
    "MINI_BATCH_SIZE = 2**11\n",
    "PRINT_INTERVAL = 1 #and interval for summary writer\n",
    "SAVER_INTERVAL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('hyperparameter'):\n",
    "    #need keep_prob placeholder since it should be 1 during prediction\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    #summaries for hyperparameter\n",
    "    keep_prob_summ = tf.summary.scalar('keep_prob', keep_prob)\n",
    "    seed_summ = tf.summary.scalar('seed', tf.convert_to_tensor(SEED))\n",
    "    lstm_dim_summ = tf.summary.scalar('lstm_dim',\n",
    "                                      tf.convert_to_tensor(LSTM_DIM))\n",
    "    decay_step_summ = tf.summary.scalar('decay_step',\n",
    "                                    tf.convert_to_tensor(DECAY_STEP))\n",
    "    decay_rate_summ = tf.summary.scalar('decay_rate',\n",
    "                                    tf.convert_to_tensor(DECAY_RATE))\n",
    "    grad_max_summ = tf.summary.scalar('grad_max',\n",
    "                                     tf.convert_to_tensor(GRAD_MAX))\n",
    "    data_size_summ = tf.summary.scalar('data_size',\n",
    "                                tf.convert_to_tensor(DATA_SIZE))\n",
    "    tag_size_summ = tf.summary.scalar('tag_size',\n",
    "                                tf.convert_to_tensor(TAG_SIZE))\n",
    "    shape_size_summ = tf.summary.scalar('shape_size',\n",
    "                                tf.convert_to_tensor(SHAPE_SIZE))\n",
    "    mini_batch_summ = tf.summary.scalar('mini_batch_size',\n",
    "                                tf.convert_to_tensor(MINI_BATCH_SIZE))\n",
    "    seq_length_summ = tf.summary.scalar('seq_length',\n",
    "                                      tf.convert_to_tensor(SEQ_LENGTH))\n",
    "    token_length_summ = tf.summary.scalar('token_length',\n",
    "                                    tf.convert_to_tensor(TOKEN_LENGTH))\n",
    "    char_size_summ = tf.summary.scalar('char_size',\n",
    "                                      tf.convert_to_tensor(CHAR_SIZE))\n",
    "    char_dim_summ = tf.summary.scalar('char_dim',\n",
    "                                      tf.convert_to_tensor(CHAR_DIM))\n",
    "    comb_char_summ = tf.summary.scalar('comb_char',\n",
    "                                      tf.convert_to_tensor(COMB_CHAR))\n",
    "    out_char_summ = tf.summary.scalar('out_char',\n",
    "                                      tf.convert_to_tensor(OUT_CHAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to write notes of the experiment I am going to do. So here it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = 'W1: Xavier; b1: zeros; ' + \\\n",
    "        'loss: crf log likelihood; ' + \\\n",
    "        'optimizer: adam'\n",
    "comments = tf.placeholder(dtype=tf.string) #will be filled later\n",
    "with tf.name_scope('experiment_notes'):\n",
    "    notes_summ = tf.summary.text('notes', tf.convert_to_tensor(notes))\n",
    "    comments_summ = tf.summary.text('comments', comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create placeholder for the data I have prepared before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    X_input = tf.placeholder(name='X_input', dtype=tf.int32,\n",
    "                         shape=[None, SEQ_LENGTH])\n",
    "    X_word_shape = tf.placeholder(name='X_word_shape', dtype=tf.int32,\n",
    "                                  shape=[None, SEQ_LENGTH])\n",
    "    X_char_input = tf.placeholder(name='X_char_input', dtype=tf.int32,\n",
    "                                  shape=[None, SEQ_LENGTH,\n",
    "                                         TOKEN_LENGTH])\n",
    "    Y_input = tf.placeholder(name='Y_input', dtype=tf.int32,\n",
    "                         shape=[None, SEQ_LENGTH])\n",
    "    seq_length_input = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    mask_input = tf.cast(tf.sign(X_input), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initialize character embedding randomly using **Xavier Initializer**. Then, the char vectors are inputted to **convolution** and **max pool** layers. Later, the result is appended to word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('char'):\n",
    "    char_embed = tf.get_variable(name='char_embed', shape=[CHAR_SIZE,\n",
    "                            CHAR_DIM], initializer=tf.contrib.layers \\\n",
    "                            .xavier_initializer(seed=SEED))\n",
    "    X_char = tf.nn.embedding_lookup(char_embed, X_char_input)\n",
    "    char_filter = tf.get_variable(name='char_filter', shape=[1,\n",
    "                    COMB_CHAR, CHAR_DIM, OUT_CHAR], initializer= \\\n",
    "                    tf.contrib.layers.xavier_initializer(seed=SEED))\n",
    "    conv = tf.nn.conv2d(X_char, char_filter, [1, 1, 1, 1], 'VALID')\n",
    "    maxpool = tf.nn.max_pool(conv, [1, 1, TOKEN_LENGTH-COMB_CHAR+1,\n",
    "                                    1], [1, 1, 1, 1], 'VALID')\n",
    "    maxpool = tf.reshape(maxpool,shape= [-1, SEQ_LENGTH, OUT_CHAR])\n",
    "    \n",
    "    #summary\n",
    "    char_embed_summ = tf.summary.histogram('char_embed_summary',\n",
    "                                           char_embed)\n",
    "    char_filter_summ = tf.summary.histogram('char_filter_summary',\n",
    "                                            char_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding is initialized using the pre-trained Word2Vec. However, the embedding is still trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('word'):\n",
    "    word_embed = tf.Variable(name='word_embed',\n",
    "                             initial_value=word_vec_embed,\n",
    "                             trainable=True, dtype=tf.float32)\n",
    "    X_word = tf.nn.embedding_lookup(word_embed, X_input)\n",
    "    X_word = tf.concat([tf.one_hot(X_word_shape, SHAPE_SIZE), X_word],\n",
    "                       axis=2)\n",
    "#     X_embed = X_word\n",
    "    X_embed = tf.concat([X_word, maxpool], axis=2, name='X_embed')\n",
    "    \n",
    "    #summary\n",
    "    word_embed_summ = tf.summary.histogram('word_embed_summary',\n",
    "                                           word_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initialize weights and biases for next layer after LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('weights'):\n",
    "    W1 = tf.get_variable(name='W1', shape=[2*LSTM_DIM, TAG_SIZE],\n",
    "                         initializer=tf.contrib.layers \\\n",
    "                                     .xavier_initializer(seed=SEED))\n",
    "    b1 = tf.get_variable(name='b1', shape=[TAG_SIZE],\n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    #summary\n",
    "    W1_summ = tf.summary.histogram('W1_summary', W1)\n",
    "    b1_summ = tf.summary.histogram('b1_summary', b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use basic LSTM with default activation (**tanh**) and dropout layer for forward and backward cells. Then, those cells are inputted to **bidirectional dynamic RNN**. That is why the sequence length is necessary. After that, the output of LSTM are multiplied by the weights and then added by biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('lstm'):\n",
    "    cell_fw = tf.nn.rnn_cell.BasicLSTMCell(num_units=LSTM_DIM)\n",
    "    cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw, seed=SEED,\n",
    "                                            output_keep_prob=keep_prob)\n",
    "    cell_bw = tf.nn.rnn_cell.BasicLSTMCell(num_units=LSTM_DIM)\n",
    "    cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw, seed=SEED,\n",
    "                                            output_keep_prob=keep_prob)\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw,\n",
    "                        X_embed, seq_length_input, dtype=tf.float32)\n",
    "    outputs = tf.concat([outputs[0], outputs[1]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('output'):\n",
    "    seq_outputs = tf.reshape(tf.nn.xw_plus_b(tf.reshape( \\\n",
    "                    outputs, shape=[-1, 2*LSTM_DIM]),\n",
    "                    W1, b1), shape=[-1, SEQ_LENGTH, TAG_SIZE],\n",
    "                    name='seq_outputs')\n",
    "    softmax = tf.nn.softmax(seq_outputs)\n",
    "    #prediction without crf\n",
    "    seq_predict = tf.cast(tf.argmax(softmax, axis=2), tf.float32,\n",
    "                          name='seq_predict')\n",
    "    \n",
    "    #summary\n",
    "    seq_outputs_summ = tf.summary.histogram('seq_outputs_summary',\n",
    "                                            seq_outputs)\n",
    "    seq_pred_summ = tf.summary.histogram('seq_predict_summary',\n",
    "                                         seq_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CRF** module in tensorflow is pretty amazing! I just need to write few words of code to calculate the log likelihood and the predicted sequence. Note that **transition matrix** here is the probability of moving from one tag to other tag and **vetrebi sequence** is the predicted sequence of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    log_likelihood, transition_matrix = tf.contrib.crf \\\n",
    "        .crf_log_likelihood(seq_outputs, Y_input, seq_length_input)\n",
    "    loss = tf.reduce_mean(-log_likelihood)\n",
    "    viterbi_seq, viterbi_score = tf.contrib.crf.crf_decode( \\\n",
    "        seq_outputs, transition_matrix, seq_length_input)\n",
    "\n",
    "    #summary\n",
    "    loss_summ = tf.summary.scalar('loss_function', loss)\n",
    "    trans_matrix_summ = tf.summary.histogram('trans_matrix_summ',\n",
    "                                             transition_matrix)\n",
    "    viterbi_seq_summ = tf.summary.histogram('viterbi_seq_summary',\n",
    "                                             viterbi_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I initialize the training. I use **Adam** for my optimizer. I understand that Adam can adjust the learning rate, but when I tried to extract the learning rate from Adam, the learning rate just did not change. Thus, to make sure I use learning rate with **exponential decay**. I also use **gradient clipping** to ensure the learning is not too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    inc_global_step_op = global_step.assign_add(1)\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE,\n",
    "                                        global_step, DECAY_STEP,\n",
    "                                        DECAY_RATE, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, name='optimizer')\n",
    "    grads, vars = zip(*optimizer.compute_gradients(loss))\n",
    "    capped_grads, _ = tf.clip_by_global_norm(grads, GRAD_MAX)\n",
    "    train_op = optimizer.apply_gradients(zip(capped_grads, vars))\n",
    "\n",
    "    #summary\n",
    "    learning_summ = tf.summary.scalar('learning_rate', learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use 2 accuracy measures. First is the **sentence accuracy**. I think the measure is too strict since the prediction sequence has to be completely equal to the true sequence. Moreover, the sentence lengths are various and some sentences might contain many OOV words. Therefore, I use another measure which is **token accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    viterbi_seq = tf.multiply(viterbi_seq, mask_input)\n",
    "    output_equal = tf.cast(tf.equal(tf.cast(viterbi_seq, tf.int32),\n",
    "                        Y_input), dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.reduce_min(output_equal, axis=1),\n",
    "                              name='acc')\n",
    "    accuracy_token = tf.reduce_sum(tf.multiply(output_equal,\n",
    "                        tf.cast(mask_input, tf.float32)))/tf.cast( \\\n",
    "                        tf.reduce_sum(mask_input), tf.float32)\n",
    "\n",
    "    #summary\n",
    "    #I create separate summary for train and test\n",
    "    train_acc_summ = tf.summary.scalar('train_accuracy', accuracy)\n",
    "    test_acc_summ = tf.summary.scalar('test_accuracy', accuracy)\n",
    "    train_acc_tkn_summ = tf.summary.scalar('train_token_accuracy',\n",
    "                                           accuracy_token)\n",
    "    test_acc_tkn_summ = tf.summary.scalar('test_token_accuracy',\n",
    "                                          accuracy_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I create merger object for summary and saver object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('summary'):\n",
    "    train_merger = tf.summary.merge([train_acc_summ, loss_summ,\n",
    "                                     train_acc_tkn_summ,\n",
    "                                     learning_summ])\n",
    "    test_merger = tf.summary.merge([test_acc_summ,\n",
    "                                    test_acc_tkn_summ])\n",
    "    histo_merger = tf.summary.merge([word_embed_summ, W1_summ, b1_summ,\n",
    "                                     char_embed_summ, char_filter_summ,\n",
    "                                     seq_outputs_summ, seq_pred_summ,\n",
    "                                     viterbi_seq_summ,\n",
    "                                     trans_matrix_summ])\n",
    "    hyper_merger = tf.summary.merge([seed_summ, lstm_dim_summ,\n",
    "                                data_size_summ, shape_size_summ,\n",
    "                                tag_size_summ, grad_max_summ,\n",
    "                                mini_batch_summ, keep_prob_summ,\n",
    "                                decay_rate_summ, decay_step_summ,\n",
    "                                seq_length_summ, token_length_summ,\n",
    "                                char_size_summ, char_dim_summ, \n",
    "                                comb_char_summ, out_char_summ])\n",
    "    notes_merger = tf.summary.merge([notes_summ])\n",
    "    comments_merger = tf.summary.merge([comments_summ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('saver'):\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "Let's start training the network! First, I prepare the writer object and set common seed to ensure reproduceable. I also prepare separate dictionary for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save target\n",
    "LOG_DIR = 'INPUT-YOUR-LOG-DIRECTORY-HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {X_input: X_train, X_word_shape: X_ws_train,\n",
    "              Y_input: Y_train, seq_length_input: seq_length_train,\n",
    "              X_char_input: X_char_train, keep_prob: 1.0}\n",
    "test_dict = {X_input: X_test, X_word_shape: X_ws_test,\n",
    "             Y_input: Y_test, seq_length_input: seq_length_test,\n",
    "             X_char_input: X_char_test, keep_prob: 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment before training\n",
    "start_comments = 'best setup'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training algorithm. I use shuffle the data each epoch to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "step 1: 0.17 mins\n",
      "step 2: 0.17 mins\n",
      "step 3: 0.19 mins\n",
      "step 4: 0.17 mins\n",
      "step 5: 0.16 mins\n",
      "step 6: 0.19 mins\n",
      "step 7: 0.16 mins\n",
      "step 8: 0.17 mins\n",
      "step 9: 0.17 mins\n",
      "step 10: 0.19 mins\n",
      "step 11: 0.16 mins\n",
      "step 12: 0.17 mins\n",
      "step 13: 0.19 mins\n",
      "step 14: 0.18 mins\n",
      "step 15: 0.18 mins\n",
      "step 16: 0.21 mins\n"
     ]
    }
   ],
   "source": [
    "# writing notes\n",
    "summ = sess.run(hyper_merger, {keep_prob: KEEP_PROB})\n",
    "writer.add_summary(summ, 0)\n",
    "summ = sess.run(notes_merger)\n",
    "writer.add_summary(summ, 0)\n",
    "summ = sess.run(comments_merger, {comments: start_comments})\n",
    "writer.add_summary(summ, 0)\n",
    "\n",
    "# training\n",
    "summ = sess.run(train_merger, train_dict)\n",
    "writer.add_summary(summ, 0)\n",
    "summ = sess.run(histo_merger, train_dict)\n",
    "writer.add_summary(summ, 0)\n",
    "summ = sess.run(test_merger, test_dict)\n",
    "writer.add_summary(summ, 0)\n",
    "print 'step 0'\n",
    "START_TIME = time.time()\n",
    "\n",
    "for step in range(1, EPOCHS+1):\n",
    "    r = np.random.permutation(DATA_SIZE) #to shuffle the data\n",
    "    for no_batch in range(int(1.0*DATA_SIZE/MINI_BATCH_SIZE + 0.99)):\n",
    "        start = no_batch*MINI_BATCH_SIZE\n",
    "        end = min((no_batch+1)*MINI_BATCH_SIZE, DATA_SIZE)\n",
    "        batch_dict = {X_input: X_train[r][start: end],\n",
    "                    X_word_shape: X_ws_train[r][start: end],\n",
    "                    Y_input: Y_train[r][start: end],\n",
    "                    seq_length_input: seq_length_train[r][start: end],\n",
    "                    X_char_input: X_char_train[r][start: end],\n",
    "                    keep_prob: KEEP_PROB}\n",
    "        sess.run(train_op, batch_dict)\n",
    "    i_step = sess.run(inc_global_step_op)\n",
    "    \n",
    "    if step%PRINT_INTERVAL == 0:\n",
    "        summ = sess.run(train_merger, train_dict)\n",
    "        writer.add_summary(summ, step)\n",
    "        summ = sess.run(histo_merger, train_dict)\n",
    "        writer.add_summary(summ, step)\n",
    "        summ = sess.run(test_merger, test_dict)\n",
    "        writer.add_summary(summ, step)\n",
    "        print 'step {0}: {1:.2f} mins'.format(step,\n",
    "                                        (time.time() - START_TIME)/60)\n",
    "        START_TIME = time.time()\n",
    "\n",
    "    if step%SAVER_INTERVAL == 0:\n",
    "        saver.save(sess, LOG_DIR + 'model', global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Network\n",
    "To evaluate the network, I have to observe the log in the tensorboard. So here I evaluate the performance empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kita' 'lihat' 'daya beli' 'masyarakat' 'masih' 'batas' '' 'indikasi'\n",
      " '-nya' 'rasio' 'tabung' 'hadap' 'pdb' 'cenderung' 'turun' '']\n",
      "['PRP' 'VB' 'NN' 'NN' 'MD' 'JJ' 'Z' 'NN' 'PRP' 'NN' 'NN' 'IN' 'NN' 'JJ'\n",
      " 'VB' 'Z']\n",
      "['PRP' 'VB' 'NN' 'NN' 'MD' 'VB' 'Z' 'VB' 'PRP' 'NN' 'NN' 'IN' 'NN' 'JJ'\n",
      " 'VB' 'Z']\n",
      "['juru bicara' 'nato' 'tolak' 'beri' 'detail' 'jadi' '' 'dan' 'kata'\n",
      " 'bahwa' 'dengan' 'sebut' 'provinsi' 'dapat' 'ungkap' 'kewarganegaraan'\n",
      " 'prajurit' 'itu' '']\n",
      "['NN' 'NNP' 'VB' 'VB' 'NN' 'NN' 'Z' 'CC' 'VB' 'SC' 'SC' 'VB' 'NN' 'MD' 'VB'\n",
      " 'NN' 'NN' 'PR' 'Z']\n",
      "['NN' 'NNP' 'VB' 'VB' 'NN' 'VB' 'Z' 'CC' 'VB' 'SC' 'IN' 'PR' 'NN' 'MD' 'VB'\n",
      " 'NN' 'NN' 'PR' 'Z']\n"
     ]
    }
   ],
   "source": [
    "# print incorrect prediction\n",
    "predict = sess.run(tf.cast(viterbi_seq, tf.int32), test_dict)\n",
    "mask_pred = sess.run(tf.cast(mask_input, tf.int32), test_dict)\n",
    "for i in range(len(X_test[:5])):\n",
    "    if not np.array_equal(Y_test[i], predict[i]):\n",
    "        print np.array([index2word[idx_] for idx_ in X_test[i]]) \\\n",
    "                    [mask_pred[i] == 1]\n",
    "        print tag_encoder.inverse_transform(Y_test[i]) \\\n",
    "                    [mask_pred[i] == 1] #correct sequence\n",
    "        print tag_encoder.inverse_transform(predict[i]) \\\n",
    "                    [mask_pred[i] == 1] #predicted sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the network more, I create custom function to predict POS tag of custom sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(sentence_raw):\n",
    "    sentence_raw = sentence_raw.split()\n",
    "\n",
    "    sentence_ws = [word_shape(token) for token in sentence_raw]\n",
    "    sentence_ws = tf.contrib.keras.preprocessing.sequence \\\n",
    "                        .pad_sequences([sentence_ws], SEQ_LENGTH,\n",
    "                                    padding='post', truncating='post')\n",
    "    \n",
    "    sentence = [stemmer.stem(token) for token in sentence_raw]\n",
    "    print sentence\n",
    "    \n",
    "    char_sentence = list()\n",
    "    count = 0\n",
    "    for token_ in sentence:\n",
    "        char_sentence.append(convert_token(token_))\n",
    "        count += 1\n",
    "        if count == SEQ_LENGTH-1:\n",
    "            break\n",
    "    for i in range(SEQ_LENGTH-count):\n",
    "        char_sentence.append(convert_token('<PAD>'))\n",
    "    char_sentence = np.array(char_sentence).reshape(-1, SEQ_LENGTH,\n",
    "                                                    TOKEN_LENGTH)\n",
    "\n",
    "    sentence = [word2index[token] if token in word2index else 1\n",
    "                for token in sentence]\n",
    "    print sentence\n",
    "    sentence = tf.contrib.keras.preprocessing.sequence \\\n",
    "                        .pad_sequences([sentence], SEQ_LENGTH,\n",
    "                                    padding='post', truncating='post')\n",
    "\n",
    "    custom_dict = {X_input: sentence, X_word_shape: sentence_ws,\n",
    "                   seq_length_input: np.array([len(sentence_raw)]),\n",
    "                   X_char_input: char_sentence,\n",
    "                   keep_prob:1}\n",
    "    predictor = sess.run(viterbi_seq, custom_dict)\n",
    "    masked = sess.run(mask_input, custom_dict)\n",
    "    return tag_encoder.inverse_transform(predictor[0])[masked[0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saya', 'pergi', 'ke', 'pasar', '']\n",
      "[189, 1100, 53, 70, 2]\n",
      "['PRP' 'VB' 'IN' 'NN' 'Z']\n"
     ]
    }
   ],
   "source": [
    "stc = 'Saya pergi ke pasar .'\n",
    "print custom_predict(stc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I add closing comments below consist of the result of evaluation and further experiment plan. I also save the final step model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/av170602/log/tensorflow/pos_tagger/171206/01/model-fin-16'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_comments = 'works nice'\n",
    "summ = sess.run(comments_merger, {comments: end_comments})\n",
    "writer.add_summary(summ, 1)\n",
    "writer.close()\n",
    "saver.save(sess, LOG_DIR + 'model-fin', global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all. Hope you enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:abe]",
   "language": "python",
   "name": "conda-env-abe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
