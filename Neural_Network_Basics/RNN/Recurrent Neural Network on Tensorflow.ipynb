{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network on Tensorflow\n",
    "[Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) is one of the architectures in the Neural Network. I see RNN as the 'Time Series' version of Neural Network, since the input is sequential data. Actually, I've been working with RNN in many projects before, but I've never created it from scratch before. This is the right time to create one. I will create the network based on this [paper](https://arxiv.org/pdf/1506.00019.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "I prepare simple data for my the experiment. The data is constructed using this rule: $x_t = 1.1 \\times x_{t-1} + 0.3$. I generate 1000 sample sequences and use 80:20 rule for train and test. I will train the RNN to detect this rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "SEQ_LENGTH = 10\n",
    "random_seq = np.random.normal(size=[1000, SEQ_LENGTH])\n",
    "for i in range(1, SEQ_LENGTH):\n",
    "    random_seq[:, i] = random_seq[:, i-1]*1.1 + 0.3\n",
    "random_seq = random_seq.reshape(-1, SEQ_LENGTH, 1)\n",
    "train_seq = random_seq[:800,:,:]\n",
    "test_seq = random_seq[800:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RNN\n",
    "Let's just start by creating 'vanilla' RNN. RNN is essentially similar to the Neural Network with 2 layers. Just like below image.\n",
    "<img src='NN.png'>\n",
    "However, since it deals with sequences, somehow it needs a mechanism to consider the data in the previous time step. Mathematically, we write $$\\textbf{h}^{(t)}=\\sigma(W^{hx}\\textbf{x}^{(t)}+W^{hh}\\textbf{h}^{(t-1)}+\\textbf{b}_h).$$\n",
    "where $\\textbf{h}^{(t)}$ and $\\textbf{x}^{(t)}$ are the hidden and the input nodes at time $t$, $W^{**}$ is the weight from $*$ and to $*$, and $\\sigma$ is an activation function. From the equation, it's clear that hidden state is affected by hidden state at previous time step. At first, we can just set $\\textbf{h}_0=\\textbf{0}$.\n",
    "\n",
    "Let's put this knowledge into code! I use 4 nodes in the hidden layer and 1 node in the output layer (since the output is scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "HIDDEN_DIM = 4\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the input placeholder and variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('input'):\n",
    "    seq_input = tf.placeholder(dtype=tf.float32,\n",
    "                        shape=[None,SEQ_LENGTH,1], name='seq_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('weights'):\n",
    "    W_hx = tf.get_variable(name='W_hx', shape=[1,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    W_hh = tf.get_variable(name='W_hh', shape=[HIDDEN_DIM,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    b_h = tf.get_variable(name='b_h', shape=[HIDDEN_DIM],\n",
    "                          dtype=tf.float32)\n",
    "\n",
    "    W_yh = tf.get_variable(name='W_yh', shape=[HIDDEN_DIM,1],\n",
    "                           dtype=tf.float32)\n",
    "    b_y = tf.get_variable(name='b_y', shape=[1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, initially the hidden nodes have value 0. To get all the hidden values at every time step, the hidden values are appended in the list and stacked together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnn'):\n",
    "    hidden_0 = tf.zeros(shape=[tf.shape(seq_input)[0], HIDDEN_DIM],\n",
    "                      dtype=tf.float32, name='hidden_0')\n",
    "    hidden = hidden_0\n",
    "    hiddens = list()\n",
    "    for j in range(SEQ_LENGTH-1):\n",
    "        hidden = tf.matmul(seq_input[:,j,:], W_hx) + \\\n",
    "                 tf.nn.xw_plus_b(hidden, W_hh, b_h)\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "        hiddens.append(hidden)\n",
    "    hiddens = tf.stack(hiddens, axis=1, name='hiddens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prepare output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('output'):\n",
    "    outputs = tf.reshape(hiddens, [-1,HIDDEN_DIM])\n",
    "    outputs = tf.nn.xw_plus_b(outputs, W_yh, b_y)\n",
    "    outputs = tf.reshape(outputs, [-1,SEQ_LENGTH-1,1], name='outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I simply use mean squared error as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('learning'):\n",
    "    predict = tf.identity(outputs[:,-1,:], 'predict')\n",
    "    loss = tf.losses.mean_squared_error(labels=seq_input[:,-1,:],\n",
    "                                        predictions=predict)\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1 loss: 24.8303\n",
      "step: 2 loss: 21.7922\n",
      "step: 3 loss: 19.3631\n",
      "step: 4 loss: 17.4194\n",
      "step: 5 loss: 15.8064\n",
      "step: 6 loss: 14.4164\n",
      "step: 7 loss: 13.188\n",
      "step: 8 loss: 12.0878\n",
      "step: 9 loss: 11.0972\n",
      "step: 10 loss: 10.2044\n",
      "step: 11 loss: 9.40108\n",
      "step: 12 loss: 8.67933\n",
      "step: 13 loss: 8.0307\n",
      "step: 14 loss: 7.44491\n",
      "step: 15 loss: 6.90991\n",
      "step: 16 loss: 6.41455\n",
      "step: 17 loss: 5.95542\n",
      "step: 18 loss: 5.54593\n",
      "step: 19 loss: 5.21866\n",
      "step: 20 loss: 5.00694\n",
      "step: 21 loss: 4.90687\n",
      "step: 22 loss: 4.87362\n",
      "step: 23 loss: 4.86081\n",
      "step: 24 loss: 4.83629\n",
      "step: 25 loss: 4.77895\n",
      "step: 26 loss: 4.67703\n",
      "step: 27 loss: 4.52724\n",
      "step: 28 loss: 4.33285\n",
      "step: 29 loss: 4.10137\n",
      "step: 30 loss: 3.84233\n",
      "step: 31 loss: 3.56532\n",
      "step: 32 loss: 3.27842\n",
      "step: 33 loss: 2.9873\n",
      "step: 34 loss: 2.69531\n",
      "step: 35 loss: 2.40529\n",
      "step: 36 loss: 2.12352\n",
      "step: 37 loss: 1.86426\n",
      "step: 38 loss: 1.65704\n",
      "step: 39 loss: 1.56637\n",
      "step: 40 loss: 1.65935\n",
      "step: 41 loss: 1.74606\n",
      "step: 42 loss: 1.63108\n",
      "step: 43 loss: 1.39447\n",
      "step: 44 loss: 1.17073\n",
      "step: 45 loss: 1.03002\n",
      "step: 46 loss: 0.969458\n",
      "step: 47 loss: 0.952475\n",
      "step: 48 loss: 0.941681\n",
      "step: 49 loss: 0.909916\n",
      "step: 50 loss: 0.841809\n",
      "step: 51 loss: 0.735525\n",
      "step: 52 loss: 0.606138\n",
      "step: 53 loss: 0.487062\n",
      "step: 54 loss: 0.420424\n",
      "step: 55 loss: 0.424938\n",
      "step: 56 loss: 0.459549\n",
      "step: 57 loss: 0.455043\n",
      "step: 58 loss: 0.395218\n",
      "step: 59 loss: 0.323798\n",
      "step: 60 loss: 0.287797\n",
      "step: 61 loss: 0.295435\n",
      "step: 62 loss: 0.317968\n",
      "step: 63 loss: 0.320956\n",
      "step: 64 loss: 0.291951\n",
      "step: 65 loss: 0.246051\n",
      "step: 66 loss: 0.211125\n",
      "step: 67 loss: 0.203577\n",
      "step: 68 loss: 0.212977\n",
      "step: 69 loss: 0.213346\n",
      "step: 70 loss: 0.192679\n",
      "step: 71 loss: 0.164572\n",
      "step: 72 loss: 0.149577\n",
      "step: 73 loss: 0.152348\n",
      "step: 74 loss: 0.159526\n",
      "step: 75 loss: 0.156462\n",
      "step: 76 loss: 0.142189\n",
      "step: 77 loss: 0.128307\n",
      "step: 78 loss: 0.125246\n",
      "step: 79 loss: 0.13055\n",
      "step: 80 loss: 0.132846\n",
      "step: 81 loss: 0.12661\n",
      "step: 82 loss: 0.117671\n",
      "step: 83 loss: 0.114183\n",
      "step: 84 loss: 0.116484\n",
      "step: 85 loss: 0.117827\n",
      "step: 86 loss: 0.113442\n",
      "step: 87 loss: 0.105922\n",
      "step: 88 loss: 0.101114\n",
      "step: 89 loss: 0.100654\n",
      "step: 90 loss: 0.100664\n",
      "step: 91 loss: 0.0975524\n",
      "step: 92 loss: 0.0925945\n",
      "step: 93 loss: 0.0895343\n",
      "step: 94 loss: 0.0892543\n",
      "step: 95 loss: 0.0889817\n",
      "step: 96 loss: 0.0865204\n",
      "step: 97 loss: 0.0830896\n",
      "step: 98 loss: 0.0811435\n",
      "step: 99 loss: 0.0808424\n",
      "step: 100 loss: 0.0801978\n"
     ]
    }
   ],
   "source": [
    "for step in range(1,101):\n",
    "    _ = sess.run(train_step, {seq_input: train_seq})\n",
    "    curr_loss = sess.run(loss, {seq_input: train_seq})\n",
    "    print 'step:', step, 'loss:', curr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, not so bad result in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.206641\n"
     ]
    }
   ],
   "source": [
    "prediction = sess.run(predict, {seq_input: test_seq})\n",
    "test_loss = sess.run(loss, {seq_input: test_seq})\n",
    "print 'test loss:', test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory.\n",
    "Next, I create LSTM, which is an extension of the basic RNN. Let's see the illustration.\n",
    "<img src='LSTM.png'>\n",
    "When you search LSTM in Google, most likely you will encounter this kind of image. It might be daunting at first, but don't worry. I'll explain. First, it has a kind of 'memory', called **state**. Next, the state is also accompanied with 3 'gates' that govern how the information will be written to the state and the hidden node itself. The first two gates are called the **input** ($i$) and **forget** ($f$) gate. The input gate adjusts how much of new information should be written in the state, whereas the forget gate adjusts how much of old information should be forgotten. These two gates are working together to modify the state value. The last gate is called **output** ($o$) gate. It governs how much of the state value should be output to the hidden node. Since all of the gates have sigmoid activation functions, they range between 0 and 1, which represents 'how much' effect the gates are posing. In mathematical way, the equations are as follows:\n",
    "$$\\textbf{i}^{(t)}=\\sigma(W^{ix}\\textbf{x}^{(t)}+W^{ih}\\textbf{h}^{(t-1)}+\\textbf{b}_i),$$\n",
    "$$\\textbf{f}^{(t)}=\\sigma(W^{fx}\\textbf{x}^{(t)}+W^{fh}\\textbf{h}^{(t-1)}+\\textbf{b}_f),$$\n",
    "$$\\textbf{o}^{(t)}=\\sigma(W^{ox}\\textbf{x}^{(t)}+W^{oh}\\textbf{h}^{(t-1)}+\\textbf{b}_o),$$\n",
    "$$\\textbf{g}^{(t)}=\\phi(W^{gx}\\textbf{x}^{(t)}+W^{gh}\\textbf{h}^{(t-1)}+\\textbf{b}_g),$$\n",
    "$$\\textbf{s}^{(t)}=\\textbf{g}^{(t)} \\odot \\textbf{i}^{(t)} + \\textbf{s}^{(t-1)} \\odot \\textbf{f}^{(t)},$$\n",
    "$$\\textbf{h}^{(t)}= \\phi(\\textbf{s}^{(t)}) \\odot \\textbf{o}^{(t)},$$\n",
    "where $\\textbf{s}^{(t)}$ represents the state, $\\textbf{g}^{(t)}$ the input to the state, $\\odot$ the scalar multiplication, and $\\phi$ the tanh function. Next, let's just put them into code using the same data as previous RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "HIDDEN_DIM = 4\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initialize the input and the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('input'):\n",
    "    seq_input = tf.placeholder(dtype=tf.float32,\n",
    "                        shape=[None,SEQ_LENGTH,1], name='seq_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('weights'):\n",
    "    W_gx = tf.get_variable(name='W_gx', shape=[1,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    W_gh = tf.get_variable(name='W_gh', shape=[HIDDEN_DIM,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    W_ix = tf.get_variable(name='W_ix', shape=[1,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    W_ih = tf.get_variable(name='W_ih', shape=[HIDDEN_DIM,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    W_fx = tf.get_variable(name='W_fx', shape=[1,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    W_fh = tf.get_variable(name='W_fh', shape=[HIDDEN_DIM,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    W_ox = tf.get_variable(name='W_ox', shape=[1,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    W_oh = tf.get_variable(name='W_oh', shape=[HIDDEN_DIM,HIDDEN_DIM],\n",
    "                           dtype=tf.float32)\n",
    "    b_g = tf.get_variable(name='b_g', shape=[HIDDEN_DIM],\n",
    "                          dtype=tf.float32)\n",
    "    b_i = tf.get_variable(name='b_i', shape=[HIDDEN_DIM],\n",
    "                          dtype=tf.float32)\n",
    "    b_f = tf.get_variable(name='b_f', shape=[HIDDEN_DIM],\n",
    "                          dtype=tf.float32)\n",
    "    b_o = tf.get_variable(name='b_o', shape=[HIDDEN_DIM],\n",
    "                          dtype=tf.float32)\n",
    "\n",
    "    W_yh = tf.get_variable(name='W_yh', shape=[HIDDEN_DIM,1],\n",
    "                           dtype=tf.float32)\n",
    "    b_y = tf.get_variable(name='b_y', shape=[1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mechanism is essentially similar with the basic RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnn'):\n",
    "    state_0 = tf.zeros(shape=[tf.shape(seq_input)[0], HIDDEN_DIM],\n",
    "                       dtype=tf.float32, name='state_0')\n",
    "    state = state_0\n",
    "    states = list()\n",
    "    hidden_0 = tf.zeros(shape=[tf.shape(seq_input)[0], HIDDEN_DIM],\n",
    "                        dtype=tf.float32, name='hidden_0')\n",
    "    hidden = hidden_0\n",
    "    hiddens = list()\n",
    "    for j in range(SEQ_LENGTH-1):\n",
    "        g = tf.matmul(seq_input[:,j,:], W_gx) + \\\n",
    "                 tf.nn.xw_plus_b(hidden, W_gh, b_g)\n",
    "        g = tf.tanh(g, name='g')\n",
    "        i = tf.matmul(seq_input[:,j,:], W_ix) + \\\n",
    "                 tf.nn.xw_plus_b(hidden, W_ih, b_i)\n",
    "        i = tf.sigmoid(i, name='i')\n",
    "        f = tf.matmul(seq_input[:,j,:], W_fx) + \\\n",
    "                 tf.nn.xw_plus_b(hidden, W_fh, b_f)\n",
    "        f = tf.sigmoid(g, name='f')\n",
    "        o = tf.matmul(seq_input[:,j,:], W_ox) + \\\n",
    "                 tf.nn.xw_plus_b(hidden, W_oh, b_o)\n",
    "        o = tf.sigmoid(o, name='o')\n",
    "        state = g*i + f*state\n",
    "        states.append(state)\n",
    "        hidden = o*tf.tanh(state)\n",
    "        hiddens.append(hidden)\n",
    "    states = tf.stack(states, axis=1, name='states')\n",
    "    hiddens = tf.stack(hiddens, axis=1, name='hiddens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output and learning process are similar with the previous RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('output'):\n",
    "    outputs = tf.reshape(hiddens, [-1,HIDDEN_DIM])\n",
    "    outputs = tf.nn.xw_plus_b(outputs, W_yh, b_y)\n",
    "    outputs = tf.reshape(outputs, [-1,SEQ_LENGTH-1,1], name='outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('learning'):\n",
    "    predict = tf.identity(outputs[:,-1,:], 'predict')\n",
    "    loss = tf.losses.mean_squared_error(labels=seq_input[:,-1,:],\n",
    "                                        predictions=predict)\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1 loss: 14.5623\n",
      "step: 2 loss: 12.4639\n",
      "step: 3 loss: 10.8879\n",
      "step: 4 loss: 9.56814\n",
      "step: 5 loss: 8.27149\n",
      "step: 6 loss: 7.01537\n",
      "step: 7 loss: 5.99699\n",
      "step: 8 loss: 5.29935\n",
      "step: 9 loss: 4.78822\n",
      "step: 10 loss: 4.35377\n",
      "step: 11 loss: 3.95908\n",
      "step: 12 loss: 3.59298\n",
      "step: 13 loss: 3.25045\n",
      "step: 14 loss: 2.92614\n",
      "step: 15 loss: 2.61298\n",
      "step: 16 loss: 2.30848\n",
      "step: 17 loss: 2.02133\n",
      "step: 18 loss: 1.76062\n",
      "step: 19 loss: 1.52389\n",
      "step: 20 loss: 1.31662\n",
      "step: 21 loss: 1.14633\n",
      "step: 22 loss: 1.01484\n",
      "step: 23 loss: 0.919205\n",
      "step: 24 loss: 0.854123\n",
      "step: 25 loss: 0.80886\n",
      "step: 26 loss: 0.771471\n",
      "step: 27 loss: 0.732364\n",
      "step: 28 loss: 0.685605\n",
      "step: 29 loss: 0.629251\n",
      "step: 30 loss: 0.565037\n",
      "step: 31 loss: 0.497537\n",
      "step: 32 loss: 0.431543\n",
      "step: 33 loss: 0.368814\n",
      "step: 34 loss: 0.313661\n",
      "step: 35 loss: 0.277237\n",
      "step: 36 loss: 0.260737\n",
      "step: 37 loss: 0.251952\n",
      "step: 38 loss: 0.244557\n",
      "step: 39 loss: 0.229023\n",
      "step: 40 loss: 0.208485\n",
      "step: 41 loss: 0.185181\n",
      "step: 42 loss: 0.166462\n",
      "step: 43 loss: 0.160733\n",
      "step: 44 loss: 0.157898\n",
      "step: 45 loss: 0.160293\n",
      "step: 46 loss: 0.153986\n",
      "step: 47 loss: 0.146716\n",
      "step: 48 loss: 0.133143\n",
      "step: 49 loss: 0.123948\n",
      "step: 50 loss: 0.116955\n",
      "step: 51 loss: 0.116389\n",
      "step: 52 loss: 0.117173\n",
      "step: 53 loss: 0.116464\n",
      "step: 54 loss: 0.112148\n",
      "step: 55 loss: 0.104709\n",
      "step: 56 loss: 0.098609\n",
      "step: 57 loss: 0.0941451\n",
      "step: 58 loss: 0.093101\n",
      "step: 59 loss: 0.0912236\n",
      "step: 60 loss: 0.0891725\n",
      "step: 61 loss: 0.084711\n",
      "step: 62 loss: 0.0813145\n",
      "step: 63 loss: 0.0784069\n",
      "step: 64 loss: 0.0775785\n",
      "step: 65 loss: 0.0761747\n",
      "step: 66 loss: 0.0740259\n",
      "step: 67 loss: 0.0705726\n",
      "step: 68 loss: 0.0675967\n",
      "step: 69 loss: 0.0657275\n",
      "step: 70 loss: 0.0649213\n",
      "step: 71 loss: 0.0640489\n",
      "step: 72 loss: 0.0624444\n",
      "step: 73 loss: 0.0604482\n",
      "step: 74 loss: 0.0586972\n",
      "step: 75 loss: 0.0577149\n",
      "step: 76 loss: 0.056862\n",
      "step: 77 loss: 0.0558092\n",
      "step: 78 loss: 0.0542161\n",
      "step: 79 loss: 0.0528861\n",
      "step: 80 loss: 0.051893\n",
      "step: 81 loss: 0.0513512\n",
      "step: 82 loss: 0.0505492\n",
      "step: 83 loss: 0.0495024\n",
      "step: 84 loss: 0.0483053\n",
      "step: 85 loss: 0.0473908\n",
      "step: 86 loss: 0.0466631\n",
      "step: 87 loss: 0.0459007\n",
      "step: 88 loss: 0.0449895\n",
      "step: 89 loss: 0.0440312\n",
      "step: 90 loss: 0.0432839\n",
      "step: 91 loss: 0.0426426\n",
      "step: 92 loss: 0.0419907\n",
      "step: 93 loss: 0.0411762\n",
      "step: 94 loss: 0.0403818\n",
      "step: 95 loss: 0.0396862\n",
      "step: 96 loss: 0.0390998\n",
      "step: 97 loss: 0.0384876\n",
      "step: 98 loss: 0.0378199\n",
      "step: 99 loss: 0.0371875\n",
      "step: 100 loss: 0.0366293\n"
     ]
    }
   ],
   "source": [
    "for step in range(1,101):\n",
    "    _ = sess.run(train_step, {seq_input: train_seq})\n",
    "    curr_loss = sess.run(loss, {seq_input: train_seq})\n",
    "    print 'step:', step, 'loss:', curr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same number of iteration, LSTM has better result compared to the vanilla RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.118942\n"
     ]
    }
   ],
   "source": [
    "prediction = sess.run(predict, {seq_input: test_seq})\n",
    "test_loss = sess.run(loss, {seq_input: test_seq})\n",
    "print 'test loss:', test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, that's all. I hope it helps you understanding more about RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2: merlin18 with tf1.4",
   "language": "python",
   "name": "merlin18tf1.4py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
